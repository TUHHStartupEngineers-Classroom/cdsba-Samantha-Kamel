[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "#Content:"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Assignment 4:\nGiven a set of Data:\n\n\nFor each variable, compute expected value, variance and standard deviation.\n\nExplain, if it makes sense to compare the standard deviations.\n\nThen, examine the relationship between both variables and compute the covariance and correlation\n\nWhat measure is easier to interpret? Please discuss your interpretation.\n\nCompute the conditional expected value for:\n\n\\(E[income|age&lt;=18]\\)\n\n\\(E[income|age \\in [18,65)]\\)\n\n\\(E[income|age&gt;=65]\\)\n\n\n\nSolution:\n\nCode & results:\n\n\n#Loading Libraries & files:\nlibrary(tidyverse)\nrandom_vars &lt;- readRDS(\"D:/OneDrive/Samantha/TUHH/1stsemester/Causal Data Science for Business Analytics (SE)/Causal_Data_Science_Data/random_vars.rds\")\n#Viewing the Data:\nrandom_vars\n\n\n\n  \n\n\n#Calculating the Expected Values:\nexpectedValues &lt;-summarize(random_vars, Age_Expected_Value=mean(age, na.rm = TRUE), Income_Expected_Value=mean(income, na.rm = TRUE))\nexpectedValues\n\n\n\n  \n\n\n#Calculating the Variance:\nvariances &lt;-summarize(random_vars, Age_Variance=var(age, na.rm = TRUE), Income_Variance=var(income, na.rm = TRUE))\nvariances\n\n\n\n  \n\n\n#Calculating the Standard Deviation:\nstd &lt;-summarize(random_vars, Age_Standard_Deviation=sd(age, na.rm = TRUE), Income_Standard_Deviation=sd(income, na.rm = TRUE))\nstd\n\n\n\n  \n\n\n\n\nThe standard deviation is a measure of the proportions of the data being far away from the expected value, therefore, in this case the values stand for age and income and the comparison won’t indicate anything.\nCode & results:\n\n\n#Relationships Between Variables:\ncov_corr &lt;-summarize(random_vars, Covariance=cov(age,income), Correlation=cor(age,income))\ncov_corr\n\n\n\n  \n\n\n\n\nThe covariance is the relationship between the two variables and hoe they behave to each other (Measure of linear dependency). Therefore when the value is a high positive value, it means that the two variables are moving into the same direction. But it is not possible to indicate whether the result is high or not as there is no value to compare relative to it. Therefore, using the correlation value is better as the correlation is the standardized measure and is by construction bounded between -1 and 1. The high values in magnitude (close to 1 or -1) indicates very strong linear relationship and the direction of this relation is represented by the algebraic sign.\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation: Since both values of the covariance and correlation are positive, the age and income are moving in the same direction. And using the correlation value which is 0.548, this means that relation falls in the beginning of the of the strong range of correlation.\n(According to https://www.tastylive.com/concepts-strategies/correlation#:~:text=Positive%20correlation%20is%20measured%20on,move%20in%20the%20same%20direction. Positive correlation is measured on a 0.1 to 1.0 scale. Weak positive correlation would be in the range of 0.1 to 0.3, moderate positive correlation from 0.3 to 0.5, and strong positive correlation from 0.5 to 1.0).\nThis means that when the age increases, the income increases and vice versa (not with the same exact rate).\n\n\n\nThe conditional expected values:\n\n\n\\(E[income|age&lt;=18]\\):\n\n\n#The expected value of the income when age&lt;=18:\ncondExpectedValue1&lt;-mean(filter(random_vars,age&lt;=18)$income)\ncondExpectedValue1\n\n#&gt; [1] 389.6074\n\n\n\n\\(E[income|age\\in[18,65)]\\)\n\n\n#The expected value of the income when 18&lt;=age&lt;65:\ncondExpectedValue2&lt;-mean(filter(random_vars,18&lt;=age & age&lt;65)$income)\ncondExpectedValue2\n\n#&gt; [1] 4685.734\n\n\n\n\\(E[income|age&gt;=65]\\)\n\n\n#The expected value of the income when age&gt;=65:\ncondExpectedValue3&lt;-mean(filter(random_vars,age&gt;=65)$income)\ncondExpectedValue3\n\n#&gt; [1] 1777.237"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Spurious Correlation:\nAccording to STATOLOGY:\nIn statistics, spurious correlation refers to a correlation between two variables that occurs purely by chance without one variable actually causing the other to occur.\nThis type of correlation is dangerous because it can sometimes make people think that one variable causes another, when in reality the correlation exists purely by chance.\nIt turns out that this type of correlation between variables happens all the time in real life.\n\nOne real-life example is Master’s Degrees vs. Box Office Revenue:\nIf we collect data for the total number of Master’s degrees issued by universities each year and the total box office revenue generated by year, we would find that the two variables are highly correlated.\nThis doesn’t mean that issuing more Master’s degrees is causing the box office revenue to increase each year.\nThe more likely explanation is that the global population has been increasing each year, which means more Master’s degrees are issued each year and the sheer number of people attending movies each year are both increasing in roughly equal amounts.\nThe correlation between the two variables is spurious.\n\n\n\nSpurious Correlation between Master’s Degrees & Box Office Revenue\n\n\n\n\n\nData Fabrication and Visualization:\nBy fabricating data similar to the previous one using a linear and a square root function with normally distributed noise, we would get a similar plot for this spurious correlation (because the noise used is random the plot and data will slightly change at each run but the concept and the plot shape are the same).\n\n#Load Library:\nlibrary(tidyverse)  \n\n#Fabricating Data:\nfirst_year &lt;- 1910\nlast_year &lt;- 2020 \nyears &lt;- seq(first_year,last_year, by=10)\nrand_var&lt;-900\nn &lt;- length(years)\nMasters_mean &lt;- 0\nMasters_sd &lt;- 5\nBox_Office_mean &lt;- 0\nBox_Office_sd &lt;- 20000\nMasters_data &lt;- years-rand_var+rnorm(n, Masters_mean, Masters_sd)\nBoxOffice_data &lt;- sqrt((years-rand_var)^2+rnorm(n, Box_Office_mean, Box_Office_sd))\ndata &lt;- data.frame(Year=rep(years, times=2), data_group= rep(c(\"Masters_Degree_Issued\",\"Box_Office_Revenue\"),each=n) ,Data=c(Masters_data,BoxOffice_data) )\ndata\n\n\n\n  \n\n\n#Plot:\nggplot(data,aes(x = Year, y = Data, group=data_group, color=data_group)) +  geom_line()"
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Assignment 11:\nyou have developed an app and you are already having an active user base. Of course, some users are more active than other users. Also, users might use the app for different purposes. In general, user behavior likely depends on a lot of unobserved characteristics. Obviously, your goal is to keep users as long as possible on the app to maximize your ad revenues. To do that, you want to introduce a new feature and see how it affects time spent on the app. Simply comparing users who use the newly introduced feature to users who don’t would result in a biased estimate due to the unobserved confounders regarding their activity and willingness to use a new feature. Therefore, you perform a so called randomized encouragement trial, where for a random selection of users, a popup appears when opening the app and encourages these users to test new feature. The users who are not randomly selected don’t get a popup message but could also use the new feature.\n\n\nDraw a DAG of how you understand the relationships.\n\nCompute the naive, biased estimate.\n\nFor the assumptions that can be (partly) tested, check whether they are satisfied by either computing correlations or drawing plots. Argue whether instrumental variable estimation is an adequate procedure.\n\nCompute the IV estimate using 2SLS and compare it to the naive estimate. Would you consider the naive estimate biased, and if yes, does it have an upward or downward bias?\n\n\nSolution:\n\nDAG:\n\n\n#Loading Libraries & Data\n\nlibrary(tidyverse)  \n\n\nlibrary(dagitty)  \n\n\nlibrary(ggdag)\n\n\nlibrary(estimatr)  \n\n\nrand_enc &lt;- readRDS(\"D:/OneDrive/Samantha/TUHH/1stsemester/Causal Data Science for Business Analytics (SE)/Causal_Data_Science_Data/rand_enc.rds\")\n\nhead(rand_enc)\n\n\n\n  \n\n\n#Define DAG\niv_expl &lt;- dagify(\n  Y ~ D,\n  Y ~ U,\n  D ~ U,\n  D ~ Z,\n  exposure = \"D\",\n  latent = \"U\",\n  outcome = \"Y\",\n  coords = list(x = c(U = 1, D = 0, Y = 2, Z = -1),\n                y = c(U = 1, D = 0, Y = 0, Z = 0)),\n  labels = c(\"D\" = \"Using New Feature\", \n             \"Y\" = \"Time Spent\", \n             \"U\" = \"Activity\",\n             \"Z\" = \"Randomized Encouragement Trial\")\n)\n\n#DAG\nggdag(iv_expl, text = T) +\n      geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\n\n\nNaive, Biased Estimate:\n\n\n#Direct regression\nnaive_est &lt;- lm(time_spent ~ used_ftr, data=rand_enc)\nsummary(naive_est)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = rand_enc)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\nAssumptions Check:\n\n\n#Correlation:\napp_assume &lt;- rand_enc %&gt;%\n  filter(used_ftr == 1)\n\ncor(app_assume$rand_enc,app_assume$time_spent)\n\n#&gt; [1] -0.02744462\n\n\nSince the correlation value is a very small negative value, the rand_enc and the outcome are not correlated and hence it could be used as an instrumental variable.\n\n#Correlation Matrix\ncor(rand_enc) %&gt;% round(2)\n\n#&gt;            rand_enc used_ftr time_spent\n#&gt; rand_enc       1.00     0.20       0.13\n#&gt; used_ftr       0.20     1.00       0.71\n#&gt; time_spent     0.13     0.71       1.00\n\n#Plotting\nggplot(rand_enc, aes(x = time_spent, y = used_ftr, color = as.factor(used_ftr))) +\n  geom_jitter(alpha = .5) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n  theme(legend.title = element_blank()) + ggtitle(\"Time Spent vs Used Feature\")\n\n\n\n\n\n\n\nggplot(rand_enc, aes(x = time_spent, y = rand_enc, color = as.factor(rand_enc))) +\n  geom_jitter(alpha = .5) +\n  scale_color_discrete(labels = c(\"Without Trial\", \"With Trial\")) +\n  theme(legend.title = element_blank()) + ggtitle(\"Time Spent vs Trail Application\")\n\n\n\n\n\n\n\nggplot(rand_enc, aes(x = used_ftr, y = rand_enc, color = as.factor(rand_enc))) +\n  geom_jitter(alpha = .5) +\n  scale_color_discrete(labels = c(\"Without Trial\", \"With Trial\")) +\n  theme(legend.title = element_blank()) + ggtitle(\"Used Feature vs Trail Application\")\n\n\n\n\n\n\n\n\n\n2SLS:\n\n\n#IV\nmodel_iv &lt;- iv_robust(time_spent ~ used_ftr | rand_enc, data = rand_enc)\nsummary(model_iv) \n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = rand_enc)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\nThe naive estimate is considered biased as the estimate value is higher and the bias is upward."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Assignment 9:\nYou are running an online store and one year ago, you introduced a plus membership to bind customers to your store and increase revenue. The plus memberships comes at a small cost for the customers, which is why not all of the customers subscribed. Now you want to examine whether binding customers by this membership program in fact increases your sales with subscribed customers. But of course, there are potentially confounding variables such as age, sex or pre_avg_purch (previous average purchases).\n\n\nCheck the relationships between the variables and draw a DAG as you understand the relations.\n\nCompute a naive estimate of the average treatment effect.\n\nUse the following matching methods to obtain more precise estimates:\n\n(Coarsened) Exact Matching.\n\nNearest-Neighbor Matching.\n\nInverse Probability Weighting.\n\n\n\nSolution:\n\nDAG:\n\n\n#Loading Libraries & Data\n\nlibrary(tidyverse)  \n\n\nlibrary(dagitty)  \n\n\nlibrary(ggdag)  \n\n\nlibrary(MatchIt)  \n\n\n\nmembership &lt;- readRDS(\"D:/OneDrive/Samantha/TUHH/1stsemester/Causal Data Science for Business Analytics (SE)/Causal_Data_Science_Data/membership.rds\")\n\nhead(membership)\n\n\n\n  \n\n\n# Define DAG\ndag_model &lt;- 'dag {\n  bb=\"0,0,1,1\"\n  Membership [exposure,pos=\"0.1,0.4\"]\n  Purchases [outcome,pos=\"0.5,0.4\"]\n  Age [pos=\"0.3,0.8\"]\n  Sex [pos=\"0.3,0.2\"]\n  \"Prev Purchases\" [pos=\"0.3,0.6\"]\n  Membership -&gt; Purchases\n  \"Prev Purchases\" -&gt; Membership\n  \"Prev Purchases\" -&gt; Purchases\n  Age -&gt; Membership\n  Age -&gt; Purchases\n  Sex -&gt; Membership\n  Sex -&gt; Purchases\n}'\n\n# DAG \nggdag_adjustment_set(dag_model, shadow = T, text = F) +\n  guides(color = \"none\") +  # Turn off legend\n  geom_dag_label_repel(aes(label = name))\n\n\n\n\n\n\n\n\n\nNaive Estimate of the ATE:\n\n\n# Naive estimation (not accounting for backdoors)\nmodel_naive &lt;- lm( avg_purch~ card, data = membership)\nsummary(model_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = membership)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.684   -0.199   20.424  120.166 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  65.9397     0.3965  166.29   &lt;2e-16 ***\n#&gt; card         25.2195     0.6095   41.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.11 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\nMatching Methods:\n\n(Coarsened) Exact Matching:\n\n\n\n#(Coarsened) Exact Matching:\ncem &lt;- matchit(card ~ age + sex + pre_avg_purch,\n               data = membership, \n               method = 'cem', \n               estimand = 'ATE')\n# Covariate balance\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + sex + pre_avg_purch, data = membership, \n#&gt;     method = \"cem\", estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; sex             0.0086\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 40.1743       40.1557          0.0014     0.9993    0.0016\n#&gt; sex                  0.5040        0.5040          0.0000          .    0.0000\n#&gt; pre_avg_purch       70.4611       70.0938          0.0141     0.9929    0.0044\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0064          0.1222\n#&gt; sex             0.0000          0.0000\n#&gt; pre_avg_purch   0.0130          0.1558\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 5429.65    3844\n#&gt; Matched       5716.      4164\n#&gt; Unmatched       52.        68\n#&gt; Discarded        0.         0\n\n# Use matched data\ndf_cem &lt;- match.data(cem)\nmodel_cem &lt;- lm(avg_purch~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -159.349  -20.459   -0.151   19.863  161.528 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  69.9896     0.3984  175.66   &lt;2e-16 ***\n#&gt; card         15.2043     0.6137   24.77   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.12 on 9878 degrees of freedom\n#&gt; Multiple R-squared:  0.0585, Adjusted R-squared:  0.0584 \n#&gt; F-statistic: 613.7 on 1 and 9878 DF,  p-value: &lt; 2.2e-16\n\n\n\nNearest-Neighbor Matching:\n\n\n#Nearest-Neighbor Matching: \nnn &lt;- matchit(card ~ age + sex + pre_avg_purch,\n              data = membership,\n              method = \"nearest\", \n              distance = \"mahalanobis\", \n              replace = T)\n# Covariate Balance\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + sex + pre_avg_purch, data = membership, \n#&gt;     method = \"nearest\", distance = \"mahalanobis\", replace = T)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; sex             0.0086\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       41.9964          0.0026     1.0171    0.0014\n#&gt; sex                  0.5087        0.5087          0.0000          .    0.0000\n#&gt; pre_avg_purch       76.3938       76.2937          0.0038     1.0178    0.0012\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0061          0.0281\n#&gt; sex             0.0000          0.0000\n#&gt; pre_avg_purch   0.0076          0.0301\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 1992.19    4232\n#&gt; Matched       2677.      4232\n#&gt; Unmatched     3091.         0\n#&gt; Discarded        0.         0\n\n# Use matched data\ndf_nn &lt;- match.data(nn)\nmodel_nn &lt;- lm(avg_purch~ card, data = df_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -132.730  -21.288   -1.675   18.318  146.631 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5634     0.5881  130.19   &lt;2e-16 ***\n#&gt; card         14.5957     0.7514   19.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.43 on 6907 degrees of freedom\n#&gt; Multiple R-squared:  0.05179,    Adjusted R-squared:  0.05166 \n#&gt; F-statistic: 377.3 on 1 and 6907 DF,  p-value: &lt; 2.2e-16\n\n\n\nInverse Probability Weighting:\n\n\n#Inverse Probability Weighting:\n# (1) Propensity scores\nmodel_prop &lt;- glm(card ~ age + sex + pre_avg_purch,\n                  data = membership,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + sex + pre_avg_purch, family = binomial(link = \"logit\"), \n#&gt;     data = membership)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4298676  0.0752043 -19.013   &lt;2e-16 ***\n#&gt; age            0.0011486  0.0017761   0.647    0.518    \n#&gt; sex            0.0359388  0.0412622   0.871    0.384    \n#&gt; pre_avg_purch  0.0148262  0.0009264  16.003   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13249  on 9996  degrees of freedom\n#&gt; AIC: 13257\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# Add propensities to table\ndf_aug &lt;- membership %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n# Extend data by IPW scores\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\ndf_ipw %&gt;% \n  select(card, age, sex, pre_avg_purch, propensity, ipw)\n\n\n\n  \n\n\n# (2) Estimation\nmodel_ipw &lt;- lm(avg_purch~ card,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -205.353  -28.995   -0.275   28.787  214.307 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2628     0.4320  162.66   &lt;2e-16 ***\n#&gt; card         14.9573     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.19 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05657,    Adjusted R-squared:  0.05647 \n#&gt; F-statistic: 599.5 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n# Run with high weights excluded\nmodel_ipw_trim &lt;- lm(avg_purch~ card,\n                     data = df_ipw %&gt;% filter(propensity %&gt;% between(0.15, 0.85)),\n                     weights = ipw)\nsummary(model_ipw_trim)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw %&gt;% filter(propensity %&gt;% \n#&gt;     between(0.15, 0.85)), weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -205.353  -28.995   -0.275   28.787  214.307 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2628     0.4320  162.66   &lt;2e-16 ***\n#&gt; card         14.9573     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.19 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05657,    Adjusted R-squared:  0.05647 \n#&gt; F-statistic: 599.5 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "Assignment 7:\n\n\nThe parking spots example could be summerized by th following DAG:\n\n\n# Load packages\n\nlibrary(dagitty)\n\nlibrary(ggdag)\n\n# create DAG from dagitty\ndag_model &lt;- 'dag {\nbb=\"0,0,1,1\"\nParking [exposure,pos=\"0.1,0.5\"]\nSales [outcome,pos=\"0.5,0.5\"]\nA [pos=\"0.3,0.1\"]\nB [pos=\"0.3,0.3\"]\nC [pos=\"0.3,0.5\"]\nD [pos=\"0.3,0.7\"]\nLocation [pos=\"0.3,1\"]\nParking -&gt; A\nParking -&gt; B\nParking -&gt; C\nParking -&gt; D\nA -&gt; Sales\nB -&gt; Sales\nC -&gt; Sales\nD -&gt; Sales\nLocation -&gt; Parking\nLocation -&gt; Sales\n}\n'\n# draw DAG\nggdag_status(dag_model)\n\n\n\n\n\n\n\n\n\nLoading SaaS company Data and linear regression:\n\n\n#Loading Library & Data:\n\nlibrary(tidyverse)  \n\n\ncustomer_sat &lt;- readRDS(\"D:/OneDrive/Samantha/TUHH/1stsemester/Causal Data Science for Business Analytics (SE)/Causal_Data_Science_Data/customer_sat.rds\")\n\nhead(customer_sat)\n\n\n\n  \n\n\n#Linear Regression on follow_ups:\nlm_followups &lt;- lm(satisfaction ~ follow_ups , data=customer_sat) \nsummary(lm_followups)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n#Linear Regression on follow_ups & account for subscription:\n\nlm_followups_sub &lt;- lm(satisfaction ~ . , data=customer_sat) \nsummary(lm_followups_sub)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ ., data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\n\nComparing coefficients & Explanation:\n\nFor the regression on follow_ups only, the estimate value for follow_ups is negative indicating a negative relation (the higher the number of follow_ups, the lower the satisfaction). However, including the subscription type as a factor changed the results totally as now the estimate value for the follow_ups now became positive indicating a positive relation.\nOne possible explanation could be that comparing the general relation between the follow up calls and the satisfaction is not the right indicator as it doesn’t take into account that the number of follow ups, the services provided, the nature of support during the call and many other factors may differ based on the type of subscription. Therefore, the true relation was revealed when the subscription type was considered. One other explanation after examining the following plots would be that each subscription type has a different range for follow ups number and satisfaction level. This indicates that the satisfaction level has a positive relation with number of follow_ups within each subscription category.\n\nData Visualization:\n\n\n#Satisfaction vs Follow_ups:\nggplot(customer_sat,aes(x = satisfaction, y = follow_ups)) +  geom_point() +\n  stat_smooth(method = \"lm\", se = F)\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#Satisfaction vs Follow_ups categorized by the subscription:\nggplot(customer_sat,aes(x = satisfaction, y = follow_ups, group=subscription, color=subscription)) +  geom_point() +\n  stat_smooth(method = \"lm\", se = F)\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Assignment 8:\nGiven a set of Data about a randomized experiment run by an online shop. E-commerce websites frequently conduct numerous randomized experiments, commonly referred to as AB testing in a business context.\n\n\nCheck whether the covariates are balanced across the groups. Use a plot to show it.\n\nRun a regression to find the effect of chatbot on sales.\n\nFind subgroup-specific effects by including an interaction. Compute a CATE for one exemplary group.\n\nUse the outcome variable purchase and run a logistic regression and interpret the coefficient for chatbot.\n\n\nSolution:\n\nDifferences Plots: to check the balance of the covariates, their differences are plotted for both groups (with/ without chatbot).\n\n\n#Load Library & data:\n\nlibrary(tidyverse)\n\n\nabtest_online &lt;- readRDS(\"D:/OneDrive/Samantha/TUHH/1stsemester/Causal Data Science for Business Analytics (SE)/Causal_Data_Science_Data/abtest_online.rds\")\n\nhead(abtest_online)\n\n\n\n  \n\n\n#Previous Visits difference plot:\ncompare_previsit &lt;- \n  ggplot(abtest_online, \n         aes(x = chatbot, \n             y = previous_visit, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Previous Visit\", title = \"Difference in previous visits\")\n\ncompare_previsit\n\n\n\n\n\n\n\n#Mobile Device users difference plot:\ncompare_mobiledevice &lt;- \n  ggplot(abtest_online, \n         aes(x = chatbot, \n             y = mobile_device, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Mobile Device\", title = \"Difference in mobile users\")\n\ncompare_mobiledevice\n\n\n\n\n\n\n\n#Purchase difference plot:\ncompare_purchase &lt;- \n  ggplot(abtest_online, \n         aes(x = chatbot, \n             y = purchase, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Purchase\", title = \"Difference in purchases\")\n\ncompare_purchase\n\n\n\n\n\n\n\n#Purchases amount difference plot:\ncompare_purchaseamount &lt;- \n  ggplot(abtest_online, \n         aes(x = chatbot, \n             y = purchase_amount, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Purchase Amount\", title = \"Difference in purchases amount\")\n\ncompare_purchaseamount\n\n\n\n\n\n\n\n\nAs illustrated by the plots the covariates are not balanced.\n\nThe effect of chatbot on sales:\n\n\n#Linear Regression on Chatbot:\nlm_chatbot &lt;- lm(purchase_amount ~ chatbot, data=abtest_online)\nsummary(lm_chatbot)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = abtest_online)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbotTRUE  -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\nBy checking the estimate value, having a chatbot has a negative relation with the purchases amount (sales).\n\nMobile users sub-group effect:\n\n\n#Linear Regression on Chatbot in interaction with mobile users:\nlm_chatbot_mobusr &lt;- lm(purchase_amount ~ chatbot* mobile_device, data=abtest_online)\nsummary(lm_chatbot_mobusr)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * mobile_device, data = abtest_online)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -16.98 -14.54  -9.95  14.13  65.24 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    16.9797     1.0152  16.725   &lt;2e-16 ***\n#&gt; chatbotTRUE                    -7.0301     1.4284  -4.922    1e-06 ***\n#&gt; mobile_deviceTRUE              -0.8727     1.7987  -0.485    0.628    \n#&gt; chatbotTRUE:mobile_deviceTRUE  -0.1526     2.5369  -0.060    0.952    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.66 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.03534,    Adjusted R-squared:  0.03244 \n#&gt; F-statistic: 12.16 on 3 and 996 DF,  p-value: 8.034e-08\n\n\nNegative estimate value indicates negative relation.\n\nLogistic regression of purchase on chatbot:\n\n\nlogistic_regression &lt;- glm(purchase ~ chatbot, family=binomial(link='logit'), abtest_online)\nsummary(logistic_regression)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot, family = binomial(link = \"logit\"), \n#&gt;     data = abtest_online)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.01613    0.08981  -0.180    0.857    \n#&gt; chatbotTRUE -0.98939    0.13484  -7.337 2.18e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1273.3  on 998  degrees of freedom\n#&gt; AIC: 1277.3\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\nThe estimate value for ChatbotTrue is negative which means that having a chatbot decreases the chances of purchasing from a user and the p-value is less than 0.5 meaning this has a statistically significant effect. \\(\\beta = -0.98939\\) then \\(e^\\beta = e^{-0.98939} = 0.372\\) and this means having a chatbot is associated with a \\(62.8\\% (1-0.372=0.628)\\) reduction in the purchase possibility."
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Assignment 10:\nImagine, you are manager of a large health provider that manages many hospitals and you want to test how a new admission procedure affects patient satisfaction. You randomly selected 18 hospitals that introduced the new admission procedure and compare them to 28 other hospitals that did not introduce the method. For both groups of hospitals you collected data from before and after the introduction. The data you have collected is from patient surveys where they were asked how satisfied they are. Perform a difference-in-differences analysis by:\n\n\nManually computing the mean satisfaction for treated and control hospitals before and after the treatment. Helpful functions could be filter(), pull() and basic arithmetic operations.\nUsing a linear regression to compute the estimate. Also, include group and time fixed effects in the regression, i.e. one regressor for each month and one regressor for each hospital: Consider, whether you want to include them as:\n\n\nmonth + hospital or as\n\nas.factor(month) + as.factor(hospital)\n\nand explain what the difference is.\n\nSolution:\n\nDifference-in-Differences:\n\n\n#Load Libraries & Data\n\nlibrary(tidyverse)  \n\n\nhospdd &lt;- readRDS(\"D:/OneDrive/Samantha/TUHH/1stsemester/Causal Data Science for Business Analytics (SE)/Causal_Data_Science_Data/hospdd.rds\")\n\nglimpse(hospdd)\n\n#&gt; Rows: 7,368\n#&gt; Columns: 5\n#&gt; $ hospital  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#&gt; $ frequency &lt;int&gt; 3, 2, 4, 2, 1, 1, 2, 4, 2, 2, 4, 1, 2, 3, 4, 2, 1, 2, 4, 2, …\n#&gt; $ month     &lt;int&gt; 7, 3, 2, 4, 3, 7, 4, 1, 3, 1, 1, 4, 6, 1, 1, 4, 1, 2, 4, 6, …\n#&gt; $ procedure &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, …\n#&gt; $ satis     &lt;dbl&gt; 4.106527, 3.319475, 3.411720, 3.004025, 3.110720, 2.882164, …\n\n#DID\nbefore_control &lt;- mean(hospdd %&gt;%\n  filter(procedure == 0, hospital&gt;18 & hospital&lt;= 46) %&gt;% \n  pull(satis))\n\n\nbefore_treatment &lt;- mean(hospdd %&gt;%\n  filter(procedure == 0, hospital&gt;= 1 & hospital&lt;=18) %&gt;% \n  pull(satis))\n\n\ndiff_before &lt;- before_treatment - before_control\n\nafter_control &lt;- mean(hospdd %&gt;%\n  filter(procedure == 1, hospital&gt;18 & hospital&lt;= 46) %&gt;% \n  pull(satis))\n\nafter_control&lt;- mean(rep(0,dim(hospdd)[1]))\n\nafter_treatment &lt;- mean(hospdd %&gt;%\n  filter(procedure == 1, hospital&gt;= 1 & hospital&lt;=18) %&gt;% \n  pull(satis))\n\ndiff_after &lt;- after_treatment - after_control\n\ndiff &lt;- diff_after - diff_before\n\ndiff\n\n#&gt; [1] 4.225468\n\n\n\nRegression:\n\n\nlm_dat &lt;- lm(satis ~ month + hospital, data=hospdd)\nsummary(lm_dat)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ month + hospital, data = hospdd)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.3831 -0.6724 -0.0838  0.5778  5.7881 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  3.7597212  0.0308145  122.01   &lt;2e-16 ***\n#&gt; month        0.0720728  0.0055957   12.88   &lt;2e-16 ***\n#&gt; hospital    -0.0175982  0.0008732  -20.16   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.017 on 7365 degrees of freedom\n#&gt; Multiple R-squared:  0.07208,    Adjusted R-squared:  0.07183 \n#&gt; F-statistic: 286.1 on 2 and 7365 DF,  p-value: &lt; 2.2e-16\n\nlm_dat_as_factor &lt;- lm(satis ~ as.factor(month) + as.factor(hospital), data=hospdd)\nsummary(lm_dat_as_factor)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ as.factor(month) + as.factor(hospital), \n#&gt;     data = hospdd)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.4357 -0.4930 -0.0120  0.4755  4.5398 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            3.419332   0.057597  59.367  &lt; 2e-16 ***\n#&gt; as.factor(month)2     -0.009608   0.030411  -0.316 0.752069    \n#&gt; as.factor(month)3      0.021969   0.030411   0.722 0.470083    \n#&gt; as.factor(month)4      0.349354   0.030411  11.488  &lt; 2e-16 ***\n#&gt; as.factor(month)5      0.343235   0.030411  11.286  &lt; 2e-16 ***\n#&gt; as.factor(month)6      0.348800   0.030411  11.469  &lt; 2e-16 ***\n#&gt; as.factor(month)7      0.341444   0.030411  11.228  &lt; 2e-16 ***\n#&gt; as.factor(hospital)2   0.408566   0.080413   5.081 3.85e-07 ***\n#&gt; as.factor(hospital)3   0.533625   0.082596   6.461 1.11e-10 ***\n#&gt; as.factor(hospital)4   0.227510   0.076977   2.956 0.003131 ** \n#&gt; as.factor(hospital)5  -0.145353   0.076977  -1.888 0.059030 .  \n#&gt; as.factor(hospital)6   0.447863   0.076977   5.818 6.20e-09 ***\n#&gt; as.factor(hospital)7   1.404416   0.074390  18.879  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8   0.071876   0.079452   0.905 0.365685    \n#&gt; as.factor(hospital)9  -1.518515   0.081457 -18.642  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10  1.682845   0.080413  20.927  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11  0.220965   0.079452   2.781 0.005431 ** \n#&gt; as.factor(hospital)12 -0.095303   0.081457  -1.170 0.242047    \n#&gt; as.factor(hospital)13  0.495593   0.078564   6.308 2.99e-10 ***\n#&gt; as.factor(hospital)14  0.233043   0.082596   2.821 0.004793 ** \n#&gt; as.factor(hospital)15 -0.144494   0.082596  -1.749 0.080263 .  \n#&gt; as.factor(hospital)16  1.414268   0.080413  17.588  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17  0.423543   0.083843   5.052 4.49e-07 ***\n#&gt; as.factor(hospital)18  0.153276   0.097668   1.569 0.116609    \n#&gt; as.factor(hospital)19 -1.169296   0.082596 -14.157  &lt; 2e-16 ***\n#&gt; as.factor(hospital)20 -0.376607   0.080413  -4.683 2.87e-06 ***\n#&gt; as.factor(hospital)21  0.770343   0.085215   9.040  &lt; 2e-16 ***\n#&gt; as.factor(hospital)22  0.375321   0.083843   4.476 7.70e-06 ***\n#&gt; as.factor(hospital)23  0.277726   0.082596   3.362 0.000776 ***\n#&gt; as.factor(hospital)24 -0.732120   0.088421  -8.280  &lt; 2e-16 ***\n#&gt; as.factor(hospital)25  0.222480   0.094875   2.345 0.019055 *  \n#&gt; as.factor(hospital)26 -0.209747   0.080413  -2.608 0.009116 ** \n#&gt; as.factor(hospital)27 -0.822648   0.077742 -10.582  &lt; 2e-16 ***\n#&gt; as.factor(hospital)28  0.288001   0.085215   3.380 0.000729 ***\n#&gt; as.factor(hospital)29 -0.175443   0.081457  -2.154 0.031288 *  \n#&gt; as.factor(hospital)30 -0.591916   0.097668  -6.060 1.42e-09 ***\n#&gt; as.factor(hospital)31  0.088091   0.080413   1.095 0.273344    \n#&gt; as.factor(hospital)32 -0.747340   0.081457  -9.175  &lt; 2e-16 ***\n#&gt; as.factor(hospital)33 -0.877969   0.080413 -10.918  &lt; 2e-16 ***\n#&gt; as.factor(hospital)34 -0.424406   0.075599  -5.614 2.05e-08 ***\n#&gt; as.factor(hospital)35 -0.069883   0.077742  -0.899 0.368729    \n#&gt; as.factor(hospital)36  1.714149   0.078564  21.818  &lt; 2e-16 ***\n#&gt; as.factor(hospital)37 -0.283590   0.094875  -2.989 0.002807 ** \n#&gt; as.factor(hospital)38 -0.510800   0.079452  -6.429 1.36e-10 ***\n#&gt; as.factor(hospital)39 -0.447491   0.083843  -5.337 9.72e-08 ***\n#&gt; as.factor(hospital)40  0.697539   0.079452   8.779  &lt; 2e-16 ***\n#&gt; as.factor(hospital)41 -0.573729   0.077742  -7.380 1.76e-13 ***\n#&gt; as.factor(hospital)42  0.457143   0.086733   5.271 1.40e-07 ***\n#&gt; as.factor(hospital)43 -1.196426   0.082596 -14.485  &lt; 2e-16 ***\n#&gt; as.factor(hospital)44 -0.389582   0.092446  -4.214 2.54e-05 ***\n#&gt; as.factor(hospital)45 -0.637743   0.077742  -8.203 2.74e-16 ***\n#&gt; as.factor(hospital)46 -0.345502   0.083843  -4.121 3.82e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7536 on 7316 degrees of freedom\n#&gt; Multiple R-squared:  0.4941, Adjusted R-squared:  0.4905 \n#&gt; F-statistic: 140.1 on 51 and 7316 DF,  p-value: &lt; 2.2e-16\n\n\nThe second regression (using as.factor()) makes more sense as it tests the different hospitals on the different months that’s why it is more meaningful. While, the first regression contains all hospitals throughout the whole year which is not useful."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "1 Basic Rules of Probability\n\nIf \\(A\\) is an event –&gt; \\(\\overline{A}\\) is the complement\nwith \\(P(A)+P(\\overline{A})=1\\)\nImpossible Event: \\(P(A)=0\\), Certain Event: \\(P(A)=1\\)\nProbability is a real number greater or equal to 0\nTotal probability is equal to 1\n\\(\\cup\\) : Union/ or , \\(\\cap\\) : Intersection/ and\nNon-Mutually Exclusive: \\(P(A \\cup B)= P(A) + P(B) - P(A \\cap B)\\) –&gt; Addition Rule\n\n\n\n\nVenn Diagram of Non-Mutually Exclusive Events\n\n\n\nMutually Exclusive: \\(P(A \\cup B)= P(A) + P(B)\\) as the events can’t happen at the same time \\(P(A \\cap B)=0\\)\n\n\n\n\nVenn Diagram of Mutually Exclusive Events\n\n\n\nConditional Probability:\n\\(P(A|B)\\) is the probability of \\(A\\) happening given that \\(B\\) has happened\nIf \\(A , B\\) are independent: \\(P(A|B)= P(A)\\) –&gt; Stochastic Independent\n\\(P(A \\cap B)= P(A|B) * P(B)= P(B|A) * P(A)\\)\nthen \\(P(A|B)= \\frac{P(A \\cap B)}{P(B)}\\) –&gt; Multiplication Rule\n\n\n\n2 Probability Tree\n\nBranches from 1 Node sum to 1\nProbability of two consecutive events is obtained by multiplying the probabilities\n\n\nAssignment 1:\nGiven the following Probability Tree and with defining being on time as event \\(T\\), being not on time as \\(\\overline{T}\\), having a change in scope as \\(S\\) and having no change in scope as \\(\\overline{S}\\)\n\n\n\n\n\n\n\nSolution:\n\\(P(T\\cap S)= 0.3* 0.2=0.6\\)\n\\(P(T\\cap \\overline{S})= 0.7* 0.6=0.42\\)\n\\(P(\\overline{T}\\cap S)= 0.3* 0.8=0.24\\)\n\\(P(\\overline{T}\\cap \\overline{S})= 0.7* 0.4=0.28\\)\n\n\n\n3 Set Theory\n\nVenn Diagrams are used to visualize the occurrence & relationships between events\n\n\n\n\nGeneric Venn Diagram\n\n\n\nAssignment 2:\nUsing a random sample of 1000 cistomers of application development company that has developed an app available on either Smartphones “\\(S\\)”, Tablets “\\(T\\)” or Computers “\\(C\\)”. Given the Venn Diagram of the number of customers using each device.\n\n\n\n\n\n\n\nSolution:\n\nThe percentage of customers using all three devices: \\(P(S\\cap T\\cap C)=0.5\\%\\)\nThe percentage of customers using at least two devices: \\(P(S\\cap T) + P(T\\cap C) + P(C\\cap S)+ P(S\\cap T\\cap C)=7.3+3.3+8.8+0.5=19.9\\%\\)\nThe percentage of customers using only one device: \\(= 42.3 + 27.8+ 10=80.1\\%\\)\nor \\(100 - P\\)(at least 2 devices)\\(= 100-19.9=80.1\\%\\)\n\n\n\n\n4 Bayes Theorem\n\\(P(A|B)= \\frac{P(A|B)*P(A)}{P(B)}= \\frac{P(A|B)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\\) –&gt; Bayesian Rule\n\nAssignment 3:\nYou are quality assurance manager and you want to buy a new tool that automates part of the quality assurance. If the tool finds a product it considers faulty, an alarm is triggered. The seller of the tool states that if a product is faulty, the tool is 97% reliable and if the product is flawless, the test is 99% reliable. Also, from your past experience you know that 4% of your products come out with flaws.\n\n\nGivens:\n\\(P(B|A)=0.97\\)\n\\(P(B|\\overline{A})=0.01\\)\n\\(P(\\overline{B}|\\overline{A})=0.99\\)\n\\(P(A)=0.04\\)\n\\(P(\\overline{A})=0.96\\)\n\n\nSolution:\nUsing the Bayesian Rule:\n\n\\(P(\\overline{A}|B)=\\frac{0.01*0.96}{(0.01*0.96)+(0.97*0.04)}= 0.1983471074 =19.83\\%\\)\n\\(P(A|B)=\\frac{0.97*0.04}{(0.01*0.96)+(0.97*0.04)}= 0.8016528926 =80.17\\%\\)\nVerification: \\(P(\\overline{A}|B)+P(A|B)=19.83+80.17=100\\%\\)\n\nThese results show that in case the alarm is triggered, there is a possibility of about 19.83% that the product is flawless and a probability of 80.17% that the product is faulty."
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Assignment 5:\nGiven a set of Data:\n\n\nRead the data and check the dimensions. How many rows and how many columns does the data have?\n\nUse appropriate commands to get a more detailed look at the data. What data types do you see? How do numbers differ from strings regarding their data type?\n\nRun a linear regression. You want to explain what factors are relevant for the pricing of a car.\n\nChoose one regressor and:\n\nexplain what data type it is and what values it can take on\n\nwhat effect is has on the price and what changing the value would have as a result\n\nwhether its effect is statistically significant.\n\n\nAdd a variable seat_heating to the data and assign a value TRUE for all observations. Assign it to a new object and run a regression. What coefficient do you get for the new variable seat_heating and how can you explain it?\n\n\nSolution:\n\nCode & results:\n\n\n# Load tidyerse package & Data:\nlibrary(tidyverse)  \n\ncar_prices &lt;- readRDS(\"D:/OneDrive/Samantha/TUHH/1stsemester/Causal Data Science for Business Analytics (SE)/Causal_Data_Science_Data/car_prices.rds\")\n\n#Data Details:\ndata_dimension&lt;-dim(car_prices)\nsprintf(\"Number of Rows (factors): %d\",data_dimension[1])\n\n#&gt; [1] \"Number of Rows (factors): 181\"\n\nsprintf(\"Number of Columns (cars): %d\",data_dimension[2])\n\n#&gt; [1] \"Number of Columns (cars): 22\"\n\n\n\nData Viewing:\nBy having a look at the following data. The factors used are either characters or numeric values. The 2 data types available are chr (character) for strings and dbl (double) for numbers.\n\n\n#View Data & check Data Types:\nglimpse(car_prices)\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", \"…\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", \"…\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fwd…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\",…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 105…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192.…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4,…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9,…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086,…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"…\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five\"…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 108…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13,…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40,…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.30…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 101…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500,…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, 2…\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, 2…\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 152…\n\nsapply(car_prices, class)\n\n#&gt;       aspiration       doornumber          carbody       drivewheel \n#&gt;      \"character\"      \"character\"      \"character\"      \"character\" \n#&gt;   enginelocation        wheelbase        carlength         carwidth \n#&gt;      \"character\"        \"numeric\"        \"numeric\"        \"numeric\" \n#&gt;        carheight       curbweight       enginetype   cylindernumber \n#&gt;        \"numeric\"        \"numeric\"      \"character\"      \"character\" \n#&gt;       enginesize       fuelsystem        boreratio           stroke \n#&gt;        \"numeric\"      \"character\"        \"numeric\"        \"numeric\" \n#&gt; compressionratio       horsepower          peakrpm          citympg \n#&gt;        \"numeric\"        \"numeric\"        \"numeric\"        \"numeric\" \n#&gt;       highwaympg            price \n#&gt;        \"numeric\"        \"numeric\"\n\n\n\nLinear Regression:\n\n\n#Linear Regression: \nlm_dat_original &lt;- lm(price ~ ., data=car_prices)\nsummary(lm_dat_original)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = car_prices)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nBased on the p-value (a low p-value provides support for the claim that the alternative hypothesis is true instead of the null hypothesis.The alternative hypothesis states, that there is indeed a correlation between the independent and the dependent variable. Because the p-value is low, we reject the null hypothesis and the alternative hypothesis (significant correlation) is true) resulting from the previous summary:\n\nThe most significant factors, with the least p-value, affecting the car prices are:\n\n\n\nenginetypeohc\n\nenginetypeohcv\n\ncylindernumberfive\n\ncylindernumberfour\n\nenginesize\n\nstroke\n\npeakrpm\n\n\nThese factors are significant as well because the p-value is still less than \\(\\alpha =0.05\\) but not as significant as the previous ones:\n\n\n\ncarbodyhardtop\n\ncarwidth\n\ncylindernumbersix\n\ncylindernumbertwelve\n\n\n\n\n\n\n\nNote\n\n\n\nCombining the previous results would summarize the most relevant factors for the pricing of a car to:\n\n\n\nFactor\nEstimate\np-value\n\n\n\n\nEngine Size\n125.934\n\\(5.00*10^{-6}\\)\n\n\nStroke\n-4527.137\n\\(2.49*10^{-6}\\)\n\n\nPeak RPM\n2.526\n\\(1.08*10^{-4}\\)\n\n\n\nThe Engine type could be ignored as 2 types of the available in the data set are significant and the other type is with high p-value (not significant). The same applies for the Cylinder Number as 2 Cylinders are significant and the others are less significant.\n\n\n\nOne Regressor Evaluation:\n\n\nEngine Size:\nBy observing the previous results, the estimate value of the Engine size is positive and hence resulting in a positive relationship. And the p-value is very small *** indicating that this factor is significant.\n\n\nconfint(lm_dat_original, level = .95)\n\n#&gt;                              2.5 %       97.5 %\n#&gt; (Intercept)          -6.683134e+04 -5708.591461\n#&gt; aspirationturbo      -2.123031e+02  3904.715423\n#&gt; doornumbertwo        -8.880057e+02  1373.051204\n#&gt; carbodyhardtop       -6.508183e+03  -875.302661\n#&gt; carbodyhatchback     -5.792190e+03  -896.479709\n#&gt; carbodysedan         -4.973242e+03   387.602227\n#&gt; carbodywagon         -6.373755e+03  -482.087396\n#&gt; drivewheelfwd        -2.632717e+03  1623.588215\n#&gt; drivewheelrwd        -2.522030e+03  2491.138949\n#&gt; enginelocationrear    1.558895e+03 11728.088662\n#&gt; wheelbase            -2.135865e+02   153.193109\n#&gt; carlength            -1.318793e+02    72.399687\n#&gt; carwidth              2.484517e+02  1215.185989\n#&gt; carheight            -1.428819e+02   389.272214\n#&gt; curbweight           -9.086424e-01     6.132334\n#&gt; enginetypedohcv      -1.793062e+04   846.708835\n#&gt; enginetypel          -2.552383e+03  4509.878243\n#&gt; enginetypeohc         1.500996e+03  5189.507548\n#&gt; enginetypeohcf       -2.240453e+03  4186.291965\n#&gt; enginetypeohcv       -8.666335e+03 -3778.310183\n#&gt; cylindernumberfive   -1.769255e+04 -5756.526294\n#&gt; cylindernumberfour   -1.782963e+04 -5269.025514\n#&gt; cylindernumbersix    -1.159348e+04 -2709.316066\n#&gt; cylindernumberthree  -1.358731e+04  4949.451134\n#&gt; cylindernumbertwelve -1.941739e+04 -2827.032165\n#&gt; enginesize            7.347203e+01   178.396665\n#&gt; fuelsystem2bbl       -1.569500e+03  1923.771576\n#&gt; fuelsystemmfi        -8.134945e+03  2052.909867\n#&gt; fuelsystemmpfi       -1.620436e+03  2338.991505\n#&gt; fuelsystemspdi       -5.239202e+03   151.421288\n#&gt; fuelsystemspfi       -4.425440e+03  5454.972678\n#&gt; boreratio            -4.552905e+03  1939.424404\n#&gt; stroke               -6.351094e+03 -2703.179801\n#&gt; compressionratio     -1.836863e+03   361.059971\n#&gt; horsepower           -3.459530e+01    55.182371\n#&gt; peakrpm               1.272152e+00     3.778780\n#&gt; citympg              -4.197613e+02   239.056414\n#&gt; highwaympg           -1.755422e+02   485.258537\n\nconfint(lm_dat_original, level = .9)\n\n#&gt;                                5 %          95 %\n#&gt; (Intercept)          -6.186665e+04 -10673.277432\n#&gt; aspirationturbo       1.221011e+02   3570.311210\n#&gt; doornumbertwo        -7.043517e+02   1189.397199\n#&gt; carbodyhardtop       -6.050653e+03  -1332.832509\n#&gt; carbodyhatchback     -5.394536e+03  -1294.133023\n#&gt; carbodysedan         -4.537808e+03    -47.831503\n#&gt; carbodywagon         -5.895205e+03   -960.637234\n#&gt; drivewheelfwd        -2.286999e+03   1277.870488\n#&gt; drivewheelrwd        -2.114836e+03   2083.945045\n#&gt; enginelocationrear    2.384887e+03  10902.097462\n#&gt; wheelbase            -1.837949e+02    123.401489\n#&gt; carlength            -1.152868e+02     55.807159\n#&gt; carwidth              3.269745e+02   1136.663141\n#&gt; carheight            -9.965780e+01    346.048073\n#&gt; curbweight           -3.367402e-01      5.560432\n#&gt; enginetypedohcv      -1.640544e+04   -678.477117\n#&gt; enginetypel          -1.978752e+03   3936.247160\n#&gt; enginetypeohc         1.800595e+03   4889.908750\n#&gt; enginetypeohcf       -1.718442e+03   3664.280578\n#&gt; enginetypeohcv       -8.269305e+03  -4175.339227\n#&gt; cylindernumberfive   -1.672305e+04  -6726.028378\n#&gt; cylindernumberfour   -1.680939e+04  -6289.258519\n#&gt; cylindernumbersix    -1.087187e+04  -3430.930975\n#&gt; cylindernumberthree  -1.208166e+04   3443.805514\n#&gt; cylindernumbertwelve -1.806984e+04  -4174.581121\n#&gt; enginesize            8.199451e+01    169.874176\n#&gt; fuelsystem2bbl       -1.285759e+03   1640.031137\n#&gt; fuelsystemmfi        -7.307438e+03   1225.402867\n#&gt; fuelsystemmpfi       -1.298832e+03   2017.387632\n#&gt; fuelsystemspdi       -4.801350e+03   -286.431296\n#&gt; fuelsystemspfi       -3.622905e+03   4652.437632\n#&gt; boreratio            -4.025567e+03   1412.085913\n#&gt; stroke               -6.054793e+03  -2999.481111\n#&gt; compressionratio     -1.658337e+03    182.534027\n#&gt; horsepower           -2.730312e+01     47.890194\n#&gt; peakrpm               1.475752e+00      3.575180\n#&gt; citympg              -3.662489e+02    185.544045\n#&gt; highwaympg           -1.218687e+02    431.585098\n\n\nFor extra validation the confidence intervals with different levels were obtained and the whole range for the Engine size factor was positive reflecting that it is statistically significant.\nBy repeating the linear regression with the Engine size factor only, the following results were obtained:\n\n#Engine Size Linear Regression:\nlm_engsize &lt;- lm(price ~ enginesize, data=car_prices) \nsummary(lm_engsize)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ enginesize, data = car_prices)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -10818.6  -1969.6   -168.6   1494.0  14393.9 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -8622.296    873.535  -9.871   &lt;2e-16 ***\n#&gt; enginesize    170.064      6.523  26.073   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3694 on 179 degrees of freedom\n#&gt; Multiple R-squared:  0.7916, Adjusted R-squared:  0.7904 \n#&gt; F-statistic: 679.8 on 1 and 179 DF,  p-value: &lt; 2.2e-16\n\n\nBy checking the estimate value it is positive, therefore it indicates a positive relation (when the engine size increase, the price increase).\n\nconfint(lm_engsize, level = .95)\n\n#&gt;                   2.5 %     97.5 %\n#&gt; (Intercept) -10346.0474 -6898.5441\n#&gt; enginesize     157.1932   182.9353\n\nconfint(lm_engsize, level = .9)\n\n#&gt;                     5 %       95 %\n#&gt; (Intercept) -10066.6082 -7177.9833\n#&gt; enginesize     159.2797   180.8487\n\n\nThen by checking the confidence intervals with different levels, they were completely positive and hence rejects the null hypothesis. That means that we expect an effect in the price when we change the independent variable (engine size) associated with the significant coefficient. This as well is compatible with the p-value criteria (a very low p-value). As a result: Engine size is statistically significant\nThis is also validated by plotting the results:\n\n#Plot for validation:\nggplot(lm_engsize, aes(x = price, y = enginesize)) + \ngeom_point(size = 3, alpha = 0.8)\n\n\n\n\n\n\n\n\n\nStroke:\n\n\n#Stroke Linear Regression:\nlm_stroke &lt;- lm(price ~ stroke, data=car_prices) \nsummary(lm_stroke)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ stroke, data = car_prices)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -7743  -5375  -3218   3384  32308 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  \n#&gt; (Intercept)  10504.5     6076.6   1.729   0.0856 .\n#&gt; stroke         772.4     1872.1   0.413   0.6804  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 8087 on 179 degrees of freedom\n#&gt; Multiple R-squared:  0.0009501,  Adjusted R-squared:  -0.004631 \n#&gt; F-statistic: 0.1702 on 1 and 179 DF,  p-value: 0.6804\n\nconfint(lm_stroke, level = .95)\n\n#&gt;                 2.5 %    97.5 %\n#&gt; (Intercept) -1486.438 22495.530\n#&gt; stroke      -2921.786  4466.557\n\nconfint(lm_stroke, level = .9)\n\n#&gt;                   5 %      95 %\n#&gt; (Intercept)   457.433 20551.658\n#&gt; stroke      -2322.920  3867.691\n\n#Plot for validation:\nggplot(lm_stroke, aes(x = price, y = stroke)) + \ngeom_point(size = 3, alpha = 0.8)\n\n\n\n\n\n\n\n\n\nPeak RPM:\n\n\n#Peak RPM Linear Regression:\nlm_peakrpm &lt;- lm(price ~ peakrpm, data=car_prices) \nsummary(lm_peakrpm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ peakrpm, data = car_prices)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -7994  -5379  -2914   3428  32129 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)  \n#&gt; (Intercept) 15062.3973  7417.1744   2.031   0.0438 *\n#&gt; peakrpm        -0.3981     1.4266  -0.279   0.7805  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 8089 on 179 degrees of freedom\n#&gt; Multiple R-squared:  0.0004349,  Adjusted R-squared:  -0.005149 \n#&gt; F-statistic: 0.07788 on 1 and 179 DF,  p-value: 0.7805\n\nconfint(lm_peakrpm, level = .95)\n\n#&gt;                  2.5 %       97.5 %\n#&gt; (Intercept) 426.046744 29698.747876\n#&gt; peakrpm      -3.213255     2.417027\n\nconfint(lm_peakrpm, level = .9)\n\n#&gt;                    5 %         95 %\n#&gt; (Intercept) 2798.76104 27326.033585\n#&gt; peakrpm       -2.75689     1.960661\n\n#Plot for validation:\nggplot(lm_peakrpm, aes(x = price, y = peakrpm)) + \n  geom_point(size = 3, alpha = 0.8)\n\n\n\n\n\n\n\n\nBy repeating the same steps for the 2 other factors individually: Stroke & Peak RPM are not statistically signficant as they have confidence intervals that are not completely positive or negative which supports the null hypothesis and this is also visualized by the plots.\n\nAdding the extra feature (seat_heating):\n\n\n#Extra Feature\ncar_prices_extraFeatures &lt;- car_prices %&gt;% mutate(seat_heating = TRUE)\nlm_extraFeatures &lt;- lm(price ~ ., data=car_prices_extraFeatures)\nsummary(lm_extraFeatures)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = car_prices_extraFeatures)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; seat_heatingTRUE             NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\nlm_seathaeting &lt;- lm(price ~ seat_heating, data=car_prices_extraFeatures)\nsummary(lm_seathaeting)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ seat_heating, data = car_prices_extraFeatures)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -7881  -5390  -3019   3431  32401 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)       12999.4      599.7   21.68   &lt;2e-16 ***\n#&gt; seat_heatingTRUE       NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 8068 on 180 degrees of freedom\n\n\nBy adding the extra feature seat_heating to the factors and performing the linear regression again either with all factors or with the extra factor added, we get the seat_heating coefficient as NA (missing value). This value is logical because the factor in this data set was assigned to true always and hence not affecting the price. This is also validated by the plot.\n\n#Plot for Validation:\nggplot(lm_seathaeting, aes(x = price, y = seat_heating)) + \ngeom_point(size = 3, alpha = 0.8)"
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Assignment 12:\n\nUsing the example of the coupons provided in the course content. Check how sensitive the result is to changing the bandwidth by running the analysis:\n1. with half the bandwidth\n2. with double the bandwidth\n\nNew Example: For a limited amount of time, for purchases with a total amount of more than 30€ you offered free shipping. The data can be seen in shipping.rds. Have a look at the variable purchase_amount and argue whether it could generally be used as a running variable with a cut-off at 30€.Use a plot to confirm your argument.\n\n\nSolution:\n1,2. Coupons Example:\n\n#Load Libraries & Data\n\nlibrary(dplyr)    \n\n\ndf &lt;- readRDS(\"D:/OneDrive/Samantha/TUHH/1stsemester/Causal Data Science for Business Analytics (SE)/Causal_Data_Science_Data/coupon.rds\")\nc0 &lt;- 60\n\nhead(df)\n\n\n\n  \n\n\n#Analysis with half the tested band width \nbw_half &lt;- c0 + c(-2.5, 2.5)\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_half_below &lt;- df %&gt;% filter(days_since_last &gt;= bw_half[1] & days_since_last &lt; c0)\ndf_bw_half_above &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw_half[2])\n\ndf_bw_half &lt;- bind_rows(df_bw_half_above, df_bw_half_below)\ndim(df_bw_half)\n\n#&gt; [1] 181   4\n\n# Extract values for vertical lines to visualize local average treatment effect\nmodel_bw_half_below &lt;- lm(purchase_after ~ days_since_last, df_bw_half_below)\nmodel_bw_half_above &lt;- lm(purchase_after ~ days_since_last, df_bw_half_above)\n\ny0_half &lt;- predict(model_bw_half_below, tibble(days_since_last = c0))\ny1_half &lt;- predict(model_bw_half_above, tibble(days_since_last = c0))\n\nlate_half &lt;- y1_half - y0_half\nsprintf(\"LATE of 0.5 bw: %.2f\", late_half)\n\n#&gt; [1] \"LATE of 0.5 bw: 7.36\"\n\nlm_bw_half &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw_half)\nsummary(lm_bw_half)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw_half)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -10.9680  -2.2013   0.1676   2.1516   8.2567 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.6612     0.5747  20.292  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.6883     0.3219   2.138   0.0339 *  \n#&gt; couponTRUE                 7.1679     1.0172   7.047 3.87e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.289 on 178 degrees of freedom\n#&gt; Multiple R-squared:  0.6622, Adjusted R-squared:  0.6584 \n#&gt; F-statistic: 174.5 on 2 and 178 DF,  p-value: &lt; 2.2e-16\n\n#Analysis with double the tested band width \nbw_double &lt;- c0 + c(-10, 10)\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_double_below &lt;- df %&gt;% filter(days_since_last &gt;= bw_double[1] & days_since_last &lt; c0)\ndf_bw_double_above &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw_double[2])\n\ndf_bw_double &lt;- bind_rows(df_bw_double_above, df_bw_double_below)\ndim(df_bw_double)\n\n#&gt; [1] 629   4\n\n# Extract values for vertical lines to visualize local average treatment effect\nmodel_bw_double_below &lt;- lm(purchase_after ~ days_since_last, df_bw_double_below)\nmodel_bw_double_above &lt;- lm(purchase_after ~ days_since_last, df_bw_double_above)\n\ny0_double &lt;- predict(model_bw_double_below, tibble(days_since_last = c0))\ny1_double &lt;- predict(model_bw_double_above, tibble(days_since_last = c0))\n\nlate_double &lt;- y1_double - y0_double\nsprintf(\"LATE of 2 bw: %.2f\", late_double)\n\n#&gt; [1] \"LATE of 2 bw: 9.51\"\n\nlm_bw_double &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw_double)\nsummary(lm_bw_double)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw_double)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -12.2718  -2.0858  -0.0003   2.0275  10.6749 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)              10.61700    0.27386  38.767   &lt;2e-16 ***\n#&gt; days_since_last_centered  0.01413    0.04255   0.332     0.74    \n#&gt; couponTRUE                9.51584    0.48628  19.569   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.115 on 626 degrees of freedom\n#&gt; Multiple R-squared:  0.7052, Adjusted R-squared:  0.7042 \n#&gt; F-statistic: 748.6 on 2 and 626 DF,  p-value: &lt; 2.2e-16\n\n\nDecreasing the bandwidth didn’t have significant change, however, increasing the bandwidth made the estimate value higher. Therefore we have to include the theoretical background in our Analysis.\n\nRunning Variable Check:\n\n\n#Load Libraries & Data:\n\nlibrary(rddensity)  \n\n\nshipping &lt;- readRDS(\"D:/OneDrive/Samantha/TUHH/1stsemester/Causal Data Science for Business Analytics (SE)/Causal_Data_Science_Data/shipping.rds\")\nhead(shipping)\n\n\n\n  \n\n\n# Visually check continuity at running variable\nrddd &lt;- rddensity(shipping$purchase_amount, c = 30)\nsummary(rddd)\n\n#&gt; \n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; \n#&gt; Number of obs =       6666\n#&gt; Model =               unrestricted\n#&gt; Kernel =              triangular\n#&gt; BW method =           estimated\n#&gt; VCE method =          jackknife\n#&gt; \n#&gt; c = 30                Left of c           Right of c          \n#&gt; Number of obs         3088                3578                \n#&gt; Eff. Number of obs    2221                1955                \n#&gt; Order est. (p)        2                   2                   \n#&gt; Order bias (q)        3                   3                   \n#&gt; BW est. (h)           22.909              20.394              \n#&gt; \n#&gt; Method                T                   P &gt; |T|             \n#&gt; Robust                5.9855              0\n\n\n#&gt; Warning in summary.CJMrddensity(rddd): There are repeated observations. Point\n#&gt; estimates and standard errors have been adjusted. Use option massPoints=FALSE\n#&gt; to suppress this feature.\n\n\n#&gt; \n#&gt; P-values of binomial tests (H0: p=0.5).\n#&gt; \n#&gt; Window Length / 2          &lt;c     &gt;=c    P&gt;|T|\n#&gt; 0.261                      20      26    0.4614\n#&gt; 0.522                      41      65    0.0250\n#&gt; 0.783                      62     107    0.0007\n#&gt; 1.043                      81     136    0.0002\n#&gt; 1.304                     100     169    0.0000\n#&gt; 1.565                     114     196    0.0000\n#&gt; 1.826                     132     227    0.0000\n#&gt; 2.087                     156     263    0.0000\n#&gt; 2.348                     173     298    0.0000\n#&gt; 2.609                     191     331    0.0000\n\nrdd_plot &lt;- rdplotdensity(rddd, shipping$purchase_amount, plotN = 100)\n\n\n\n\n\n\n\n\nBased on the plot, the purchase_amount can’t be used as a running variable as it is not continuous around the cutoff value. And there is no overlapping between the confedance intervals."
  },
  {
    "objectID": "index.html#assignment-i",
    "href": "index.html#assignment-i",
    "title": "My Lab Journal",
    "section": "Assignment I",
    "text": "Assignment I"
  },
  {
    "objectID": "index.html#assignment-ii",
    "href": "index.html#assignment-ii",
    "title": "My Lab Journal",
    "section": "Assignment II",
    "text": "Assignment II"
  },
  {
    "objectID": "index.html#assignment-iii",
    "href": "index.html#assignment-iii",
    "title": "My Lab Journal",
    "section": "Assignment III",
    "text": "Assignment III"
  },
  {
    "objectID": "index.html#assignment-1",
    "href": "index.html#assignment-1",
    "title": "My Lab Journal",
    "section": "1.1 Assignment 1",
    "text": "1.1 Assignment 1"
  },
  {
    "objectID": "index.html#assignment-2",
    "href": "index.html#assignment-2",
    "title": "My Lab Journal",
    "section": "1.2 Assignment 2",
    "text": "1.2 Assignment 2"
  },
  {
    "objectID": "index.html#assignment-3",
    "href": "index.html#assignment-3",
    "title": "My Lab Journal",
    "section": "1.3 Assignment 3",
    "text": "1.3 Assignment 3"
  },
  {
    "objectID": "index.html#assignment-4",
    "href": "index.html#assignment-4",
    "title": "My Lab Journal",
    "section": "2.1 Assignment 4",
    "text": "2.1 Assignment 4"
  },
  {
    "objectID": "index.html#assignment-5",
    "href": "index.html#assignment-5",
    "title": "My Lab Journal",
    "section": "3.1 Assignment 5",
    "text": "3.1 Assignment 5"
  },
  {
    "objectID": "index.html#assignment-6",
    "href": "index.html#assignment-6",
    "title": "My Lab Journal",
    "section": "4.1 Assignment 6",
    "text": "4.1 Assignment 6"
  },
  {
    "objectID": "index.html#assignment-7",
    "href": "index.html#assignment-7",
    "title": "My Lab Journal",
    "section": "5.1 Assignment 7",
    "text": "5.1 Assignment 7"
  },
  {
    "objectID": "index.html#assignment-8",
    "href": "index.html#assignment-8",
    "title": "My Lab Journal",
    "section": "6.1 Assignment 8",
    "text": "6.1 Assignment 8"
  },
  {
    "objectID": "index.html#assignment-9",
    "href": "index.html#assignment-9",
    "title": "My Lab Journal",
    "section": "7.1 Assignment 9",
    "text": "7.1 Assignment 9"
  },
  {
    "objectID": "index.html#assignment-10",
    "href": "index.html#assignment-10",
    "title": "My Lab Journal",
    "section": "8.1 Assignment 10",
    "text": "8.1 Assignment 10"
  },
  {
    "objectID": "index.html#assignment-11",
    "href": "index.html#assignment-11",
    "title": "My Lab Journal",
    "section": "9.1 Assignment 11",
    "text": "9.1 Assignment 11"
  },
  {
    "objectID": "index.html#assignment-12",
    "href": "index.html#assignment-12",
    "title": "My Lab Journal",
    "section": "10.1 Assignment 12",
    "text": "10.1 Assignment 12"
  },
  {
    "objectID": "index.html#probability-theory",
    "href": "index.html#probability-theory",
    "title": "My Lab Journal",
    "section": "1. Probability Theory",
    "text": "1. Probability Theory\n1.1 Basic Rules of Probability\n1.2 Probability Tree\n\nAssignment 1\n\n1.3 Set Theory\n\nAssignment 2\n\n1.4 Bayes Theorem\n\nAssignment 3"
  },
  {
    "objectID": "index.html#statistical-concepts",
    "href": "index.html#statistical-concepts",
    "title": "My Lab Journal",
    "section": "2. Statistical Concepts",
    "text": "2. Statistical Concepts\n2.1 Assignment 4"
  },
  {
    "objectID": "index.html#regression-and-statistical-inference",
    "href": "index.html#regression-and-statistical-inference",
    "title": "My Lab Journal",
    "section": "3. Regression and Statistical Inference",
    "text": "3. Regression and Statistical Inference\n3.1 Assignment 5"
  },
  {
    "objectID": "index.html#causality",
    "href": "index.html#causality",
    "title": "My Lab Journal",
    "section": "4. Causality",
    "text": "4. Causality\n4.1 Assignment 6"
  },
  {
    "objectID": "index.html#directed-acyclic-graphs",
    "href": "index.html#directed-acyclic-graphs",
    "title": "My Lab Journal",
    "section": "5. Directed Acyclic Graphs",
    "text": "5. Directed Acyclic Graphs\n5.1 Assignment 7"
  },
  {
    "objectID": "index.html#randomized-controlled-trials",
    "href": "index.html#randomized-controlled-trials",
    "title": "My Lab Journal",
    "section": "6. Randomized Controlled Trials",
    "text": "6. Randomized Controlled Trials\n6.1 Assignment 8"
  },
  {
    "objectID": "index.html#matching-and-subclassification",
    "href": "index.html#matching-and-subclassification",
    "title": "My Lab Journal",
    "section": "7. Matching and Subclassification",
    "text": "7. Matching and Subclassification\n7.1 Assignment 9"
  },
  {
    "objectID": "index.html#difference-in-differences",
    "href": "index.html#difference-in-differences",
    "title": "My Lab Journal",
    "section": "8. Difference-in-Differences",
    "text": "8. Difference-in-Differences\n8.1 Assignment 10"
  },
  {
    "objectID": "index.html#instrumental-variables",
    "href": "index.html#instrumental-variables",
    "title": "My Lab Journal",
    "section": "9. Instrumental variables",
    "text": "9. Instrumental variables\n9.1 Assignment 11"
  },
  {
    "objectID": "index.html#regression-discontinuity",
    "href": "index.html#regression-discontinuity",
    "title": "My Lab Journal",
    "section": "10. Regression Discontinuity",
    "text": "10. Regression Discontinuity\n10.1 Assignment 12"
  },
  {
    "objectID": "content/01_journal/01_probability.html#probability-tree",
    "href": "content/01_journal/01_probability.html#probability-tree",
    "title": "Probability Theory",
    "section": "",
    "text": "Branches from 1 Node sum to 1\nProbability of two consecutive events is obtained by multiplying the probabilities\n\n\nAssignment 1: Using the following Probability Tree and with defining being on time as event \\(T\\), being not on time as \\(\\overline{T}\\), having a change in scope as \\(S\\) and having no change in scope as \\(\\overline{S}\\)\n\n\n\n\n\n\n\\(P(T\\cap S)= 0.3* 0.2=0.6\\)\n\\(P(T\\cap \\overline{S})= 0.7* 0.6=0.42\\)\n\\(P(\\overline{T}\\cap S)= 0.3* 0.8=0.24\\)\n\\(P(\\overline{T}\\cap \\overline{S})= 0.7* 0.4=0.28\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#set-theory",
    "href": "content/01_journal/01_probability.html#set-theory",
    "title": "Probability Theory",
    "section": "",
    "text": "Assignment 2\n\n\nBayes Theorem\n\n\nAssignment 3"
  }
]